Blogs and other regularly updating websites usually have a front page with the 
most recent post as well as a Previous button on the page that takes you to the 
previous post. Then that post will also have a Previous button, and so on, creating 
a trail from the most recent page to the first post on the site. If you wanted a copy 
of the site’s content to read when you’re not online, you could manually navigate 
over every page and save each one. But this is pretty boring work, so let’s write a 
program to do it instead.


Here’s what your program does:

Loads the XKCD home page.

Saves the comic image on that page.

Follows the Previous Comic link.

Repeats until it reaches the first comic.


The output of this program will look like this:


Downloading page http://xkcd.com...
Downloading image http://imgs.xkcd.com/comics/phone_alarm.png...
Downloading page http://xkcd.com/1358/...
Downloading image http://imgs.xkcd.com/comics/nro.png...
Downloading page http://xkcd.com/1357/...
Downloading image http://imgs.xkcd.com/comics/free_speech.png...
Downloading page http://xkcd.com/1356/...
Downloading image http://imgs.xkcd.com/comics/orbital_mechanics.png...
Downloading page http://xkcd.com/1355/...
Downloading image http://imgs.xkcd.com/comics/airplane_message.png...
Downloading page http://xkcd.com/1354/...
Downloading image http://imgs.xkcd.com/comics/heartbleed_explanation.png...
--snip--
